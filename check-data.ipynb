{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Data\n",
    "\n",
    "This is a set of quick functions to generate simple dummy datasets. These will be used to adjust and \"check\" the AI anomaly detection's reliability. Once generated, they will be tested in numerous validation runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LANGCHAIN_PROJECT=\"pr-ajar-outrun-25\"\n",
    "!source .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preflight, load\n",
    "from bigstick import LoadedModel as lm\n",
    "import src.config as c\n",
    "from string import ascii_uppercase as ABC\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "import sys\n",
    "\n",
    "PROJECT = \"pr-ajar-outrun-25\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define outputs\n",
    "@dataclass\n",
    "class OutputReport:\n",
    "    \"\"\"Reporting\n",
    "    Provide a JSON file and the subsequent information\n",
    "    and this will build a report to output.\n",
    "    \"\"\"\n",
    "\n",
    "    job_name: str\n",
    "    write_dest: str = c.RESULTS_PATH\n",
    "    script_start_time: datetime = datetime.now()\n",
    "    heading: str = None\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        self.heading = {\n",
    "            \"job_name\": self.job_name,\n",
    "            \"script_start_time\": self.script_start_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        }\n",
    "\n",
    "        Path(self.write_dest).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def write(\n",
    "        self,\n",
    "        data: dict,\n",
    "        script_finish_time: datetime = datetime.now(),\n",
    "        custom_headings: dict = None,\n",
    "    ) -> Path:\n",
    "        \"\"\"Write data report\n",
    "\n",
    "        Returns:\n",
    "            Path: a Path object of the written report\n",
    "        \"\"\"\n",
    "\n",
    "        self.heading[\"script_finish_time\"] = script_finish_time.strftime(\n",
    "            \"%Y-%m-%d %H:%M:%S\"\n",
    "        )\n",
    "        self.heading = {**self.heading, **custom_headings}\n",
    "        report = {**self.heading, **{\"results\": data}}\n",
    "        report_file = f\"{self.write_dest}/report_{self.job_name}_{self.script_start_time.strftime('%Y%m%d-%H%M%S')}.json\"\n",
    "\n",
    "        with open(\n",
    "            report_file,\n",
    "            \"w+\",\n",
    "        ) as f:\n",
    "            json.dump(report, f)\n",
    "\n",
    "        return Path(report_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Dimension Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"small-dim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {}\n",
    "for letter in ABC:\n",
    "    sample[letter] = 0\n",
    "    \n",
    "sample['F'] = 1\n",
    "print(json.dumps(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Let's check this a few times._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "query_size = []\n",
    "output = OutputReport(job_name=job_name, script_start_time=datetime.now())\n",
    "c.TRIALS = 3\n",
    "for i in range(0, c.TRIALS):\n",
    "    query = f\"\"\"\n",
    "        Find any anomalies in this data: {json.dumps(sample)}.\n",
    "        Respond only with JSON containing the following keys and values:\n",
    "            - \"rank\": <the rank you assigned to the anomaly>,\n",
    "            - \"line\": <the line number of the data>,\n",
    "            - \"data\": <the relevant data>,\n",
    "            - \"explanation: <the explanation for your choice>\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "    query_size.append(sys.getsizeof(query))\n",
    "\n",
    "    resp = lm(json_mode=True, base_url=f\"http://{c.GPU_NODE}:11434\").Simple(query=query)\n",
    "    result = json.loads(resp.model_dump_json())[\"text\"].strip(\"\\n\")\n",
    "    results[i] = json.loads(result)\n",
    "\n",
    "report_loc = output.write(\n",
    "    data=results,\n",
    "    script_finish_time=datetime.now(),\n",
    "    custom_headings={\"avg_query_size\": round(sum(query_size) / len(query_size))},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = c.TRIALS\n",
    "# trials = 1000\n",
    "# report_loc = \"results/report_small-1d-array_20241012-115808.json\"\n",
    "report_data = json.load(open(report_loc, \"r\"))[\"results\"]\n",
    "\n",
    "expected_result_data = {\"F\": 1}\n",
    "raw_char_match = [\"F\", \":\", \"1\"]\n",
    "\n",
    "exact_matches = 0\n",
    "inexact_matches = 0\n",
    "inexact_matches_correct_line = 0\n",
    "non_matches = 0\n",
    "non_match_records = {}\n",
    "\n",
    "for i in report_data:\n",
    "    if all(\n",
    "        key in list(report_data[i].keys()) for key in [\"data\", \"explanation\", \"line\"]\n",
    "    ):\n",
    "        if (\n",
    "            report_data[i][\"data\"] == expected_result_data\n",
    "            and report_data[i][\"line\"] == 6\n",
    "        ):\n",
    "            exact_matches += 1\n",
    "        elif (\n",
    "            report_data[i][\"data\"] == expected_result_data\n",
    "            or \"F\" in report_data[i][\"explanation\"]\n",
    "            or all(char in str(report_data[i][\"data\"]) for char in raw_char_match)\n",
    "        ):\n",
    "            if report_data[i][\"line\"] == 6:\n",
    "                inexact_matches_correct_line += 1\n",
    "\n",
    "            else:\n",
    "                inexact_matches += 1\n",
    "\n",
    "        else:\n",
    "            non_matches += 1\n",
    "            non_match_records[i] = report_data[i]\n",
    "    elif \"anomalies\" in list(report_data[i].keys()):\n",
    "        for idx, anom in enumerate(report_data[i][\"anomalies\"]):\n",
    "            if all(key in list(anom.keys()) for key in [\"data\", \"explanation\", \"line\"]):\n",
    "                if anom[\"data\"] == expected_result_data and anom[\"line\"] == 6:\n",
    "                    exact_matches += 1\n",
    "\n",
    "                    # We break because one exact match is found\n",
    "                    # The others are wrong since we only have on PoI\n",
    "                    break\n",
    "\n",
    "                elif (\n",
    "                    anom[\"data\"] == expected_result_data\n",
    "                    or \"F\" in anom[\"explanation\"]\n",
    "                    or all(char in str(anom[\"data\"]) for char in raw_char_match)\n",
    "                ):\n",
    "                    inexact_matches += 1\n",
    "\n",
    "                    # We break because one inexact match is found\n",
    "                    # The others are wrong since we only have on PoI\n",
    "                    break\n",
    "                else:\n",
    "                    non_matches += 1\n",
    "                    non_match_records[i] = {idx: anom}\n",
    "            else:\n",
    "                non_matches += 1\n",
    "                non_match_records[i] = {idx: anom}\n",
    "    else:\n",
    "        non_matches += 1\n",
    "        non_match_records[i] = report_data[i]\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "    {exact_matches=}\n",
    "    {inexact_matches=}\n",
    "    {inexact_matches_correct_line=}\n",
    "    {non_matches=}\n",
    "    \"\"\"\n",
    ")\n",
    "print(json.dumps(non_match_records, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Dimension Varaible Data Length\n",
    "\n",
    "The small array is performant enough (around 3-5s per query) that I need to see when this performance tapers off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"small-dim-long-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacement data of interest\n",
    "replacement = \"/../../../etc/shadow\"\n",
    "\n",
    "# This file is built with jobs/log-generate.sh\n",
    "input_data = open(f\"{c.DATA_PATH}/generated-logs/generated.log\", \"r\").readlines()\n",
    "\n",
    "# We will just capture part of the log for initial analysis, basically this is a word generator\n",
    "results = enumerate([x.split(\" \")[6] for x in input_data])\n",
    "\n",
    "# Replace a random value\n",
    "ran_replace = random.choice(range(len(input_data)))\n",
    "print(f\"Find the random replacement here: {ran_replace}\")\n",
    "\n",
    "with open(f\"{c.DATA_PATH}/{job_name}.csv\", \"w+\") as f:\n",
    "    f.write(\"index,entry\\n\")\n",
    "    for k, v in results:\n",
    "        value = v \n",
    "        if k == ran_replace:\n",
    "            value = replacement\n",
    "        f.write(f\"{k},{value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "query_size = []\n",
    "input_data = open(f\"{c.DATA_PATH}/{job_name}.csv\", \"r\").read()\n",
    "output = OutputReport(job_name=job_name, script_start_time=datetime.now())\n",
    "\n",
    "trials = c.TRIALS\n",
    "# trials = 3\n",
    "\n",
    "for i in range(0, trials):\n",
    "    query = f\"\"\"\n",
    "        Find any anomalies in this data: {input_data}.\n",
    "        Respond only with JSON containing the following keys and values:\n",
    "            - \"rank\": <the rank you assigned to the anomaly>,\n",
    "            - \"line\": <the line number of the data>,\n",
    "            - \"data\": <the relevant data>,\n",
    "            - \"explanation: <the explanation for your choice>\n",
    "            \n",
    "        \"\"\"\n",
    "    resp = lm(json_mode=True, base_url=f\"http://{c.GPU_NODE}:11434\").Simple(query=query)\n",
    "\n",
    "    query_size.append(sys.getsizeof(query))\n",
    "\n",
    "    result = json.loads(resp.model_dump_json())[\"text\"].strip(\"\\n\")\n",
    "    results[i] = json.loads(result)\n",
    "\n",
    "report_loc = output.write(\n",
    "    data=results,\n",
    "    script_finish_time=datetime.now(),\n",
    "    custom_headings={\"avg_query_size\": round(sum(query_size) / len(query_size))},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_data = json.load(open(report_loc, \"r\"))[\"results\"]\n",
    "\n",
    "exact_matches = 0\n",
    "inexact_matches = 0\n",
    "inexact_matches_correct_line = 0\n",
    "non_matches = 0\n",
    "non_match_records = {}\n",
    "\n",
    "for i in report_data:\n",
    "    if all(\n",
    "        key in list(report_data[i].keys()) for key in [\"data\", \"explanation\", \"line\"]\n",
    "    ):\n",
    "        if (\n",
    "            report_data[i][\"data\"] == replacement\n",
    "            and report_data[i][\"line\"] == ran_replace\n",
    "        ):\n",
    "            exact_matches += 1\n",
    "        elif (\n",
    "            report_data[i][\"data\"] == replacement\n",
    "            or replacement in report_data[i][\"explanation\"]\n",
    "            or replacement in str(report_data[i][\"line\"])\n",
    "        ):\n",
    "            if report_data[i][\"line\"] == ran_replace:\n",
    "                inexact_matches_correct_line += 1\n",
    "            \n",
    "            else:\n",
    "                inexact_matches += 1\n",
    "\n",
    "\n",
    "        else:\n",
    "            non_matches += 1\n",
    "            non_match_records[i] = report_data[i]\n",
    "\n",
    "    elif \"anomalies\" in list(report_data[i].keys()):\n",
    "        for idx, anom in enumerate(report_data[i][\"anomalies\"]):\n",
    "            if all(key in list(anom.keys()) for key in [\"data\", \"explanation\", \"line\"]):\n",
    "                if anom[\"data\"] == replacement and anom[\"line\"] == ran_replace:\n",
    "                    exact_matches += 1\n",
    "                    break\n",
    "                elif (\n",
    "                    anom[\"data\"] == replacement\n",
    "                    or replacement in anom[\"explanation\"]\n",
    "                    or replacement in str(anom[\"line\"])\n",
    "                ):\n",
    "                    inexact_matches += 1\n",
    "                    break\n",
    "                else:\n",
    "                    non_matches += 1\n",
    "                    non_match_records[i] = {idx: anom}\n",
    "            else:\n",
    "                non_matches += 1\n",
    "                non_match_records[i] = {idx: anom}\n",
    "    else:\n",
    "        non_matches += 1\n",
    "        non_match_records[i] = report_data[i]\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "    {exact_matches=}\n",
    "    {inexact_matches=}\n",
    "    {inexact_matches_correct_line=}\n",
    "    {non_matches=}\n",
    "    \"\"\"\n",
    ")\n",
    "if len(non_match_records) > 0:\n",
    "    print(json.dumps(non_match_records, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large File with viable logs\n",
    "Through my testing I've discovered that context seems to be _required_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"apache_logs\"\n",
    "start_lines = 100\n",
    "end_lines = 100\n",
    "increment = 10\n",
    "modulo = 60\n",
    "trials = c.TRIALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacement data of interest\n",
    "replacement = '93.164.60.142 - - [17/May/2015:12:05:31 +0000] \"GET /../../../etc/shadow HTTP/1.1\" 200 32 \"-\" \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.107 Safari/537.36'\n",
    "\n",
    "\n",
    "def gen_data(lines: int = 100):\n",
    "    # This file is from Elasticsearch\n",
    "    # https://github.com/elastic/examples/blob/master/Common%20Data%20Formats/apache_logs/apache_logs\n",
    "    input_data = open(f\"{c.DATA_PATH}/apache_logs/1.txt\", \"r\").readlines()[: lines - 1]\n",
    "\n",
    "    # Modulo because % modulo\n",
    "    # ran_replace = random.choice(range((lines-int(lines*.1))-1,(lines - 1)))\n",
    "    ran_replace = 96\n",
    "\n",
    "    # Swap the data:\n",
    "    input_data[ran_replace] = f\"{replacement}\\n\"\n",
    "\n",
    "    with open(f\"{c.DATA_PATH}/{job_name}.log\", \"w+\") as f:\n",
    "        f.write(\"\".join(input_data))\n",
    "\n",
    "    return (lines, ran_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1/0\n"
     ]
    }
   ],
   "source": [
    "# Save to\n",
    "report_locs = []\n",
    "\n",
    "counter = 0\n",
    "for i in range(start_lines, end_lines + 1, increment):\n",
    "    counter += 1\n",
    "    query_size = []\n",
    "    results = {}\n",
    "\n",
    "    lines, ran_replace = gen_data(lines=i)\n",
    "    output = OutputReport(job_name=job_name, script_start_time=datetime.now())\n",
    "    input_data = open(f\"{c.DATA_PATH}/{job_name}.log\", \"r\").read()\n",
    "\n",
    "    for trial in range(0, trials):\n",
    "        query = f\"\"\"\n",
    "                Find any anomalies in this data: {input_data}.\n",
    "                Respond only with JSON containing the following keys and values:\n",
    "                    - \"rank\": <the rank you assigned to the anomaly>,\n",
    "                    - \"line\": <the line number of the data>/<the total number of lines in the file>,\n",
    "                    - \"data\": <the relevant data>,\n",
    "                    - \"explanation: <the explanation for your choice>\n",
    "                    \n",
    "                \"\"\"\n",
    "        resp = lm(json_mode=True, base_url=f\"http://{c.GPU_NODE}:11434\").Simple(\n",
    "            query=query\n",
    "        )\n",
    "\n",
    "        query_size.append(sys.getsizeof(query))\n",
    "\n",
    "        try:\n",
    "            result = json.loads(resp.model_dump_json())[\"text\"].strip(\"\\n\")\n",
    "        except ValueError as e:\n",
    "            result = {\"error\": e, \"raw\": resp.model_dump_json()}\n",
    "\n",
    "        results[trial] = json.loads(result)\n",
    "\n",
    "    report_locs.append(\n",
    "        output.write(\n",
    "            data=results,\n",
    "            script_finish_time=datetime.now(),\n",
    "            custom_headings={\n",
    "                \"avg_query_size\": round(sum(query_size) / len(query_size)),\n",
    "                \"trials\": len(results),\n",
    "                \"lines\": i,\n",
    "                \"interest\": ran_replace,\n",
    "                \"replacement\": input_data.split(\"\\n\")[ran_replace],\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "    print(f\"Completed {counter}/{round((end_lines-start_lines)/increment)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backload\n",
    "# search_s = f\"report_{job_name}_20241015\"\n",
    "# report_locs = [f for f in Path(\"./results\").rglob(f\"{search_s}*.json\")]\n",
    "\n",
    "final = {}\n",
    "\n",
    "for report_loc in report_locs:\n",
    "    report_data = json.load(open(report_loc, \"r\"))\n",
    "\n",
    "    inline = report_data[\"interest\"]\n",
    "    replacement_sub = \"/etc/shadow\"\n",
    "\n",
    "    exact_matches = 0\n",
    "    inexact_matches = 0\n",
    "    inexact_matches_correct_line = 0\n",
    "    non_matches = 0\n",
    "    non_match_records = {}\n",
    "\n",
    "    for i in report_data[\"results\"]:\n",
    "        if all(\n",
    "            key in list(report_data[\"results\"][i].keys())\n",
    "            for key in [\"data\", \"explanation\", \"line\"]\n",
    "        ):\n",
    "            if (\n",
    "                report_data[\"results\"][i][\"data\"] == replacement\n",
    "                and report_data[\"results\"][i][\"line\"] == inline\n",
    "            ):\n",
    "                exact_matches += 1\n",
    "            elif (\n",
    "                report_data[\"results\"][i][\"data\"] == replacement\n",
    "                or replacement_sub in report_data[\"results\"][i][\"explanation\"]\n",
    "                or replacement_sub in str(report_data[\"results\"][i][\"data\"])\n",
    "                or replacement_sub in str(report_data[\"results\"][i][\"line\"])\n",
    "            ):\n",
    "                if report_data[\"results\"][i][\"line\"] == inline:\n",
    "                    inexact_matches_correct_line += 1\n",
    "\n",
    "                else:\n",
    "                    inexact_matches += 1\n",
    "\n",
    "            else:\n",
    "                non_matches += 1\n",
    "                non_match_records[i] = report_data[\"results\"][i]\n",
    "\n",
    "        elif \"anomalies\" in list(report_data[\"results\"][i].keys()):\n",
    "            for idx, anom in enumerate(report_data[\"results\"][i][\"anomalies\"]):\n",
    "                if all(\n",
    "                    key in list(anom.keys()) for key in [\"data\", \"explanation\", \"line\"]\n",
    "                ):\n",
    "                    if anom[\"data\"] == replacement and anom[\"line\"] == inline:\n",
    "                        exact_matches += 1\n",
    "                        break\n",
    "                    elif (\n",
    "                        anom[\"data\"] == replacement\n",
    "                        or replacement_sub in anom[\"explanation\"]\n",
    "                        or replacement_sub in str(anom[\"data\"])\n",
    "                        or replacement_sub in str(anom[\"line\"])\n",
    "                    ):\n",
    "                        inexact_matches += 1\n",
    "                        break\n",
    "                    else:\n",
    "                        non_matches += 1\n",
    "                        non_match_records[i] = {idx: anom}\n",
    "                else:\n",
    "                    non_matches += 1\n",
    "                    non_match_records[i] = {idx: anom}\n",
    "        else:\n",
    "            non_matches += 1\n",
    "            non_match_records[i] = report_data[\"results\"][i]\n",
    "\n",
    "    dt_pat = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start = datetime.strptime(report_data[\"script_start_time\"], dt_pat)\n",
    "    end = datetime.strptime(report_data[\"script_finish_time\"], dt_pat)\n",
    "\n",
    "    final[report_data[\"lines\"]] = {\n",
    "        \"exact_matches\": exact_matches,\n",
    "        \"inexact_matches\": inexact_matches,\n",
    "        \"inexact_matches_correct_line\": inexact_matches_correct_line,\n",
    "        \"non_matches\": non_matches,\n",
    "        \"avg_query_size\": report_data[\"avg_query_size\"],\n",
    "        \"runtime\": round(end.timestamp() - start.timestamp()),\n",
    "        \"line_of_interest\": inline,\n",
    "    }\n",
    "    \n",
    "    # print(\n",
    "    #     f\"\"\"\n",
    "    #     {exact_matches=}\n",
    "    #     {inexact_matches=}\n",
    "    #     {inexact_matches_correct_line=}\n",
    "    #     {non_matches=}\n",
    "    #     \"\"\"\n",
    "    # )\n",
    "\n",
    "    # if len(non_match_records) > 0:\n",
    "    #     print(json.dumps(non_match_records, indent=4))\n",
    "\n",
    "with open(\n",
    "    f\"results/final_{job_name}_{datetime.now().strftime('%Y%m%d-%H%M%S')}.json\", \"w+\"\n",
    ") as f:\n",
    "    json.dump(final, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Check for 1d-array\n",
    "The principle here is to force the LLM to reevaluate the data. This will take the original process and simply add another check to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for i in range(0, c.TRIALS):\n",
    "    resp_1 = lm(json_mode=True, base_url=f\"http://{c.GPU_NODE}:11434\").Simple(\n",
    "        query=f\"\"\"\n",
    "        Find any anomalies in this data: {json.dumps(sample)}.\n",
    "        Respond only with JSON containing the following keys and values:\n",
    "            - \"rank\": <the rank you assigned to the anomaly>,\n",
    "            - \"line\": <the line number of the data>,\n",
    "            - \"data\": <the relevant data>,\n",
    "            - \"explanation: <the explanation for your choice>\n",
    "            \n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    result = json.loads(resp_1.model_dump_json())[\"text\"].strip(\"\\n\")\n",
    "    results[i][\"resp_1\"] = json.loads(result)\n",
    "\n",
    "    # Second query\n",
    "\n",
    "    resp_2 = lm(json_mode=True, base_url=f\"http://{c.GPU_NODE}:11434\").Simple(\n",
    "        query=f\"\"\"\n",
    "        {json.dumps(result)}\n",
    "        \n",
    "        Reivew this data. It should be formatted as a JSON dataset that contains the following keys:\n",
    "            - \"rank\": <the rank you assigned to the anomaly>,\n",
    "            - \"line\": <the line number of the data>,\n",
    "            - \"data\": <the relevant data>,\n",
    "            - \"explanation: <the explanation for your choice>\n",
    "        \n",
    "        Ensure that the formatting is correct.\n",
    "        Ensure that the values for the keys are correct by comparing to the original entry in this data:\n",
    "        {json.dumps(sample)}\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    result = json.loads(resp_2.json())[\"text\"].strip(\"\\n\")\n",
    "    results[i][\"resp_2\"] = json.loads(result)\n",
    "\n",
    "with open(f\"results/abc-1d-array_{c.TRIALS}.json\", \"w+\") as f:\n",
    "    json.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
