{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Data\n",
    "\n",
    "This is a set of quick functions to generate simple dummy datasets. These will be used to adjust and \"check\" the AI anomaly detection's reliability. Once generated, they will be tested in numerous validation runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preflight, load\n",
    "from bigstick import LoadedModel as lm\n",
    "import src.config as c\n",
    "from string import ascii_uppercase as ABC\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define outputs\n",
    "@dataclass\n",
    "class OutputReport:\n",
    "    \"\"\"Reporting\n",
    "    Provide a JSON file and the subsequent information\n",
    "    and this will build a report to output.\n",
    "    \"\"\"\n",
    "\n",
    "    job_name: str\n",
    "    write_dest: str = c.RESULTS_PATH\n",
    "    script_start_time: datetime = datetime.now()\n",
    "    heading: str = None\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        self.heading = {\n",
    "            \"job_name\": self.job_name,\n",
    "            \"script_start_time\": self.script_start_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        }\n",
    "\n",
    "        Path(self.write_dest).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def write(self, data: dict, script_finish_time: datetime = datetime.now()) -> Path:\n",
    "        \"\"\"Write data report\n",
    "\n",
    "        Returns:\n",
    "            Path: a Path object of the written report\n",
    "        \"\"\"\n",
    "\n",
    "        self.heading[\"script_finish_time\"] = script_finish_time.strftime(\n",
    "            \"%Y-%m-%d %H:%M:%S\"\n",
    "        )\n",
    "        report = {**self.heading, **{\"results\": data}}\n",
    "        report_file = f\"{self.write_dest}/report_{self.job_name}_{self.script_start_time.strftime('%Y%m%d-%H%M%S')}.json\"\n",
    "\n",
    "        with open(\n",
    "            report_file,\n",
    "            \"w+\",\n",
    "        ) as f:\n",
    "            json.dump(report, f)\n",
    "\n",
    "        return Path(report_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small 1-dimensional Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"small-1d-array\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {}\n",
    "for letter in ABC:\n",
    "    sample[letter] = 0\n",
    "    \n",
    "sample['F'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Let's check this a few times._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('results/small-1d-array_20240910-132903.json')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "output = OutputReport(job_name=job_name, script_start_time=datetime.now())\n",
    "\n",
    "for i in range(0, c.TRIALS):\n",
    "    resp = lm(json_mode=True, base_url=f\"http://{c.GPU_NODE}:11434\").Simple(\n",
    "        query=f\"\"\"\n",
    "        Find any anomalies in this data: {json.dumps(sample)}.\n",
    "        Respond only with JSON containing the following keys and values:\n",
    "            - \"rank\": <the rank you assigned to the anomaly>,\n",
    "            - \"line\": <the line number of the data>,\n",
    "            - \"data\": <the relevant data>,\n",
    "            - \"explanation: <the explanation for your choice>\n",
    "            \n",
    "        \"\"\"\n",
    "    )\n",
    "    result = json.loads(resp.model_dump_json())[\"text\"].strip(\"\\n\")\n",
    "    results[i] = json.loads(result)\n",
    "\n",
    "output.write(data=results, script_finish_time=datetime.now())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# trials = c.TRIALS\n",
    "trials = 1000\n",
    "\n",
    "results_1d_array_1000 = json.load(open(f\"results/abc-1d-array_{trials}.json\", \"r\"))\n",
    "\n",
    "expected_result_data = {\"F\": 1}\n",
    "\n",
    "exact_matches = 0\n",
    "inexact_matches = 0\n",
    "inexact_matches_correct_line = 0\n",
    "non_matches = 0\n",
    "non_match_records = {}\n",
    "\n",
    "for i in results_1d_array_1000:\n",
    "    if all(\n",
    "        key in list(results_1d_array_1000[i].keys())\n",
    "        for key in [\"data\", \"explanation\", \"line\"]\n",
    "    ):\n",
    "        if (\n",
    "            results_1d_array_1000[i][\"data\"] == expected_result_data\n",
    "            and results_1d_array_1000[i][\"line\"] == 6\n",
    "        ):\n",
    "            exact_matches += 1\n",
    "        elif (\n",
    "            results_1d_array_1000[i][\"data\"] == expected_result_data\n",
    "            or \"F\" in results_1d_array_1000[i][\"explanation\"]\n",
    "            or all(char in results_1d_array_1000[i][\"data\"] for char in [\"F\", \":\", \"1\"])\n",
    "        ):\n",
    "            inexact_matches += 1\n",
    "\n",
    "            if results_1d_array_1000[i][\"line\"] == 6:\n",
    "                inexact_matches_correct_line += 1\n",
    "\n",
    "        else:\n",
    "            non_matches += 1\n",
    "            non_match_records[i] = results_1d_array_1000[i]\n",
    "    elif \"anomalies\" in list(results_1d_array_1000[i].keys()):\n",
    "        for anom in results_1d_array_1000[i][\"anomalies\"]:\n",
    "            if all(key in list(anom.keys()) for key in [\"data\", \"explanation\", \"line\"]):\n",
    "                if anom[\"data\"] == expected_result_data and anom[\"line\"] == 6:\n",
    "                    exact_matches += 1\n",
    "                elif (\n",
    "                    anom[\"data\"] == expected_result_data\n",
    "                    or \"F\" in anom[\"explanation\"]\n",
    "                    or all(char in anom[\"data\"] for char in [\"F\", \":\", \"1\"])\n",
    "                ):\n",
    "                    inexact_matches += 1\n",
    "                else:\n",
    "                    non_matches += 1\n",
    "                    non_match_records[i] = results_1d_array_1000[i]\n",
    "            else:\n",
    "                non_matches += 1\n",
    "                non_match_records[i] = results_1d_array_1000[i]\n",
    "    else:\n",
    "        non_matches += 1\n",
    "        non_match_records[i] = results_1d_array_1000[i]\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "    {exact_matches=}\n",
    "    {inexact_matches=}\n",
    "    {inexact_matches_correct_line=}\n",
    "    {non_matches=}\n",
    "    \"\"\"\n",
    ")\n",
    "print(json.dumps(non_match_records, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large 1d-array\n",
    "\n",
    "The small array is performant enough (around 3-5s per query) that I need to see when this performance tapers off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"large-1d-array\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find the random replacement here: 596\n"
     ]
    }
   ],
   "source": [
    "# This file is built with jobs/log-generate.sh\n",
    "input_data = open(f\"{c.DATA_PATH}/generated-logs/generated.log\", \"r\").readlines()\n",
    "\n",
    "# We will just capture part of the log for initial analysis, basically this is a word generator\n",
    "results = enumerate([x.split(\" \")[6] for x in input_data])\n",
    "\n",
    "# Replace a random value\n",
    "ran_replace = random.choice(range(len(input_data)))\n",
    "\n",
    "print(f\"Find the random replacement here: {ran_replace}\")\n",
    "\n",
    "with open(f\"{c.DATA_PATH}/{job_name}.csv\", \"w+\") as f:\n",
    "    f.write(\"index,entry\\n\")\n",
    "    for k, v in results:\n",
    "        value = v \n",
    "        if k == ran_replace:\n",
    "            value = \"/../../../etc/shadow\"\n",
    "        f.write(f\"{k},{value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('results/large-1d-array_20240910-152758.json')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "input_data = open(f\"{c.DATA_PATH}/{job_name}.csv\", \"r\").read()\n",
    "output = OutputReport(job_name=job_name, script_start_time=datetime.now())\n",
    "\n",
    "for i in range(0, c.TRIALS):\n",
    "    resp = lm(json_mode=True, base_url=f\"http://{c.GPU_NODE}:11434\").Simple(\n",
    "        query=f\"\"\"\n",
    "        Find any anomalies in this data: {input_data}.\n",
    "        Respond only with JSON containing the following keys and values:\n",
    "            - \"rank\": <the rank you assigned to the anomaly>,\n",
    "            - \"line\": <the line number of the data>,\n",
    "            - \"data\": <the relevant data>,\n",
    "            - \"explanation: <the explanation for your choice>\n",
    "            \n",
    "        \"\"\"\n",
    "    )\n",
    "    result = json.loads(resp.model_dump_json())[\"text\"].strip(\"\\n\")\n",
    "    results[i] = json.loads(result)\n",
    "\n",
    "output.write(data=results, script_finish_time=datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large File with viable logs\n",
    "Through my testing I've discovered that context seems to be _required_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('results/report_apache_logs_20240913-203503.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_name = \"apache_logs\"\n",
    "results = {}\n",
    "\n",
    "input_data = open(f\"{c.DATA_PATH}/{job_name}.txt\", \"r\").read()\n",
    "output = OutputReport(job_name=job_name, script_start_time=datetime.now())\n",
    "\n",
    "# trials = c.TRIALS\n",
    "trials = 1\n",
    "\n",
    "for i in range(0, trials):\n",
    "    resp = lm(json_mode=True, base_url=f\"http://{c.GPU_NODE}:11434\").Simple(\n",
    "        query=f\"\"\"\n",
    "            Find any anomalies in this data: {input_data}.\n",
    "            Respond only with JSON containing the following keys and values:\n",
    "                - \"rank\": <the rank you assigned to the anomaly>,\n",
    "                - \"line\": <the line number of the data>/<the total number of lines in the file>,\n",
    "                - \"data\": <the relevant data>,\n",
    "                - \"explanation: <the explanation for your choice>\n",
    "                \n",
    "            \"\"\"\n",
    "    )\n",
    "    result = json.loads(resp.model_dump_json())[\"text\"].strip(\"\\n\")\n",
    "    results[i] = json.loads(result)\n",
    "\n",
    "output.write(data=results, script_finish_time=datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Check for 1d-array\n",
    "The principle here is to force the LLM to reevaluate the data. This will take the original process and simply add another check to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m      4\u001b[0m resp_1 \u001b[38;5;241m=\u001b[39m lm(json_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;241m.\u001b[39mGPU_NODE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:11434\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mSimple(\n\u001b[1;32m      5\u001b[0m     query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m    Find any anomalies in this data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson\u001b[38;5;241m.\u001b[39mdumps(sample)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m result \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(resp_1\u001b[38;5;241m.\u001b[39mjson())[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresp_1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(result)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Second query\u001b[39;00m\n\u001b[1;32m     21\u001b[0m resp_2 \u001b[38;5;241m=\u001b[39m lm(json_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;241m.\u001b[39mGPU_NODE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:11434\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mSimple(\n\u001b[1;32m     22\u001b[0m     query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson\u001b[38;5;241m.\u001b[39mdumps(result)\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     35\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for i in range(0, c.TRIALS):\n",
    "    resp_1 = lm(json_mode=True, base_url=f\"http://{c.GPU_NODE}:11434\").Simple(\n",
    "        query=f\"\"\"\n",
    "        Find any anomalies in this data: {json.dumps(sample)}.\n",
    "        Respond only with JSON containing the following keys and values:\n",
    "            - \"rank\": <the rank you assigned to the anomaly>,\n",
    "            - \"line\": <the line number of the data>,\n",
    "            - \"data\": <the relevant data>,\n",
    "            - \"explanation: <the explanation for your choice>\n",
    "            \n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    result = json.loads(resp_1.model_dump_json())[\"text\"].strip(\"\\n\")\n",
    "    results[i][\"resp_1\"] = json.loads(result)\n",
    "\n",
    "    # Second query\n",
    "\n",
    "    resp_2 = lm(json_mode=True, base_url=f\"http://{c.GPU_NODE}:11434\").Simple(\n",
    "        query=f\"\"\"\n",
    "        {json.dumps(result)}\n",
    "        \n",
    "        Reivew this data. It should be formatted as a JSON dataset that contains the following keys:\n",
    "            - \"rank\": <the rank you assigned to the anomaly>,\n",
    "            - \"line\": <the line number of the data>,\n",
    "            - \"data\": <the relevant data>,\n",
    "            - \"explanation: <the explanation for your choice>\n",
    "        \n",
    "        Ensure that the formatting is correct.\n",
    "        Ensure that the values for the keys are correct by comparing to the original entry in this data:\n",
    "        {json.dumps(sample)}\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    result = json.loads(resp_2.json())[\"text\"].strip(\"\\n\")\n",
    "    results[i][\"resp_2\"] = json.loads(result)\n",
    "\n",
    "with open(f\"results/abc-1d-array_{c.TRIALS}.json\", \"w+\") as f:\n",
    "    json.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
