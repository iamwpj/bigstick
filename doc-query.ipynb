{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain and Embeddings\n",
    "This doesn't work yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LANGCHAIN_PROJECT=\"pr-ajar-outrun-25\"\n",
    "!source .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "import src.config as c\n",
    "import json\n",
    "from uuid import uuid4\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Fix bug that loads too old of version of sqlite3.\n",
    "# https://docs.trychroma.com/troubleshooting#sqlite\n",
    "# https://gist.github.com/defulmere/8b9695e415a44271061cc8e272f3c300?permalink_comment_id=4691192#gistcomment-4691192\n",
    "# Edit .venv/lib/python3.11/site-packages/chromadb/__init__.py\n",
    "\n",
    "PROJECT=\"pr-ajar-outrun-25\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = f\"http://{c.GPU_NODE}:11434\"\n",
    "model = \"bigstick:simple\"\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=base_url,\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "# This is here for boring questions that need a quick resonse.\n",
    "# llm_general = ChatOllama(\n",
    "#     base_url=base_url,\n",
    "#     model=\"llama3.1:latest\"\n",
    "# )\n",
    "\n",
    "# https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/\n",
    "embeddings = OllamaEmbeddings(base_url=base_url, model=model)\n",
    "\n",
    "raw = open(\"./llama-data/data/apache_logs/3.txt\").read().strip().split(\"\\n\")\n",
    "\n",
    "#  https://www.geeksforgeeks.org/break-list-chunks-size-n-python/\n",
    "chunk = 500\n",
    "all_documents = [\n",
    "    ((x * chunk) // chunk, raw[x * chunk : (x + 1) * chunk])\n",
    "    for x in range((len(raw) + chunk - 1) // chunk)\n",
    "]\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"llama-data/embeddings/chroma\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"A classic greeting!\\n\\nSo, you're interested in learning more about anomalies in data sets? That's a fascinating topic!\\n\\nAnomalies can be tricky to identify, but they often indicate interesting or important patterns in the data. By ranking each item on a scale of 1-10, you can prioritize your analysis and focus on the most suspicious items first.\\n\\nWhat kind of data are you working with? Is it numerical, categorical, or something else? Do you have any specific questions about anomaly detection or would you like me to share some general tips and techniques?\", additional_kwargs={}, response_metadata={'model': 'bigstick:simple', 'created_at': '2024-10-13T14:39:48.59609906Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 7789897793, 'load_duration': 16788524, 'prompt_eval_count': 100, 'prompt_eval_duration': 477166000, 'eval_count': 114, 'eval_duration': 7172460000}, id='run-fe21e219-e350-4b10-b537-555c2f29e617-0', usage_metadata={'input_tokens': 100, 'output_tokens': 114, 'total_tokens': 214})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding document collection: 1/1\n"
     ]
    }
   ],
   "source": [
    "for index, documents in all_documents:\n",
    "    print(f\"Adding document collection: {index+1}/{len(all_documents)}\")\n",
    "\n",
    "    # Add new docs\n",
    "    uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "    metadata = {\"source\": \"apache\"}\n",
    "    docs = []\n",
    "\n",
    "    for idx, content in enumerate(documents):\n",
    "        docs.append(Document(page_content=content, id=f\"id-{idx}\", metadata=metadata))\n",
    "\n",
    "\n",
    "    collection = Chroma(\n",
    "        client=db,\n",
    "        collection_name=f\"{PROJECT}-{index}\",\n",
    "        embedding_function=embeddings,\n",
    "    )\n",
    "    collection.reset_collection()\n",
    "    collection.add_documents(documents=docs, ids=uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query for documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for http://example.com/../../../etc/passwd\n",
      "page_content='112.110.247.238 - - [17/May/2015:12:05:27 +0000] \"GET /images/googledotcom.png HTTP/1.1\" 304 - \"-\" \"Maui Browser\"' metadata={'source': 'apache'}\n",
      "page_content='66.168.50.129 - - [17/May/2015:12:05:38 +0000] \"GET /presentations/logstash-puppetconf-2012/ HTTP/1.1\" 200 37269 \"http://semicomplete.com/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:21.0) Gecko/20100101 Firefox/21.0\"' metadata={'source': 'apache'}\n",
      "page_content='50.131.51.216 - - [17/May/2015:12:05:01 +0000] \"GET /favicon.ico HTTP/1.1\" 200 3638 \"-\" \"Mozilla/5.0 (Linux; U; en-us; KFAPWI Build/JDQ39) AppleWebKit/535.19 (KHTML, like Gecko) Silk/3.13 Safari/535.19 Silk-Accelerated=true\"' metadata={'source': 'apache'}\n",
      "page_content='146.1.1.2 - - [17/May/2015:12:05:24 +0000] \"GET /favicon.ico HTTP/1.1\" 200 3638 \"-\" \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0\"' metadata={'source': 'apache'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_match = llm.invoke(\"Provide an example of a URL used in a directory traversal attack on an Apache HTTPD server. Only show unformatted example text.\").content\n",
    "\n",
    "print(f\"Looking for {log_match}\")\n",
    "results = collection.similarity_search(log_match)\n",
    "[ print(x.page_content) for x in results ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative reduction testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1/10, remaining docs: 12\n",
      "Query set to: 108.178.4.100 -...\n",
      "Deleting: 108.178.4.100 -...\n",
      "Iteration: 2/10, remaining docs: 11\n",
      "Query set to: 108.178.4.100 -...\n",
      "Deleting: 108.178.4.100 -...\n",
      "Iteration: 3/10, remaining docs: 10\n",
      "Query set to: 111.199.235.239...\n",
      "Deleting: 111.199.235.239...\n",
      "Iteration: 4/10, remaining docs: 9\n",
      "Query set to: 111.199.235.239...\n",
      "Deleting: 111.199.235.239...\n",
      "Iteration: 5/10, remaining docs: 8\n",
      "Query set to: 111.199.235.239...\n",
      "Deleting: 111.199.235.239...\n",
      "Iteration: 6/10, remaining docs: 7\n",
      "Query set to: 111.199.235.239...\n",
      "Deleting: 111.199.235.239...\n",
      "Iteration: 7/10, remaining docs: 6\n",
      "Query set to: 111.199.235.239...\n",
      "Deleting: 111.199.235.239...\n",
      "Iteration: 8/10, remaining docs: 5\n",
      "Query set to: 173.192.238.41 ...\n",
      "Deleting: 173.192.238.41 ...\n",
      "Iteration: 9/10, remaining docs: 4\n",
      "Query set to: 107.170.41.69 -...\n",
      "Deleting: 107.170.41.69 -...\n",
      "Iteration: 10/10, remaining docs: 3\n",
      "Query set to: 93.164.60.142 -...\n",
      "Deleting: 93.164.60.142 -...\n",
      "[\n",
      "    \"50.16.19.13 - - [17/May/2015:12:05:18 +0000] \\\"GET /blog/tags/puppet?flav=rss20 HTTP/1.1\\\" 200 14872 \\\"http://www.semicomplete.com/blog/tags/puppet?flav=rss20\\\" \\\"Tiny Tiny RSS/1.11 (http://tt-rss.org/)\\\"\",\n",
      "    \"180.76.5.27 - - [17/May/2015:12:05:09 +0000] \\\"GET /misc/rcfiles/ion3/look_simpleblue.lua HTTP/1.1\\\" 200 3179 \\\"-\\\" \\\"Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)\\\"\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "anomaly_count = 2\n",
    "query = \"*\"\n",
    "\n",
    "for index, _ in all_documents:\n",
    "    collection = Chroma(\n",
    "        client=db,\n",
    "        collection_name=f\"{PROJECT}-{index}\",\n",
    "        embedding_function=embeddings,\n",
    "    )\n",
    "\n",
    "    iterations = round(len(collection.get()[\"ids\"]) - anomaly_count)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        current_collection = collection.get()\n",
    "        lookup = zip(current_collection[\"ids\"], current_collection[\"documents\"])\n",
    "\n",
    "        print(\n",
    "            f\"Iteration: {i+1}/{iterations}, remaining docs: {len(current_collection['ids'])}\"\n",
    "        )\n",
    "\n",
    "        # This will help to factor out too matchy\n",
    "        return_count = len(collection.get()['ids'])\n",
    "        result = collection.similarity_search(\n",
    "            query=query,\n",
    "            k=return_count,\n",
    "        )\n",
    "\n",
    "        # We set the query to the least match\n",
    "        query = result[0].page_content\n",
    "        print(f\"Query set to: {query[:15]}...\")\n",
    "\n",
    "        # If we haven't reached our anomaly count limit then we strip the first matches out of the db.\n",
    "        if len(current_collection[\"ids\"]) >= anomaly_count:\n",
    "            uuid = [x for x, msg in lookup if msg == result[0].page_content]\n",
    "            if uuid:\n",
    "                collection.delete(uuid)\n",
    "                print(f\"Deleting: {result[0].page_content[:15]}...\")\n",
    "            else:\n",
    "                print(f\"Error when looking for uuid for {result[0].page_content[:15]}\")\n",
    "\n",
    "        # Otherwise we have fewer items left in our collection than our expected anomaly count, we have reached the end!\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "print(json.dumps(collection.get()[\"documents\"], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5c457ed5-3015-48b1-a8e1-c80c853234dc: 180.76.5.27 - - [17/May/2015:12:05:09 +0000] \"GET /misc/rcfiles/ion3/look_simpleblue.lua HTTP/1.1\" 200 3179 \"-\" \"Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)\"\n",
      "7a9e85ea-55e5-4371-bfb5-fa93639a1b1e: 180.76.5.27 - - [17/May/2015:12:05:09 +0000] \"GET /misc/rcfiles/ion3/look_simpleblue.lua HTTP/1.1\" 200 3179 \"-\" \"Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)\"\n",
      "e7662175-9833-486b-a76d-7391868c734a: 180.76.5.27 - - [17/May/2015:12:05:09 +0000] \"GET /misc/rcfiles/ion3/look_simpleblue.lua HTTP/1.1\" 200 3179 \"-\" \"Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = collection.get()\n",
    "line = '180.76.5.27 - - [17/May/2015:12:05:09 +0000] \"GET /misc/rcfiles/ion3/look_simpleblue.lua HTTP/1.1\" 200 3179 \"-\" \"Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)\"'\n",
    "lookup = zip(data['ids'], data['documents'])\n",
    "\n",
    "[ print(f\"{x}: {msg}\") for x,msg in lookup if msg == line ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 16 lines in this data. I have found the following anomalies on:\\n\\nLine 9: The request came from a user-agent identifying itself as \"Tiny Tiny RSS/1.11\", which is unusual because this software is typically used for reading RSS feeds, not making HTTP requests.\\n\\nLine 15: The request came from an IP address that resolves to a domain name belonging to the Yandex search engine, but the User-Agent string does not match any known Yandex crawler.\\n\\nNote that these anomalies may not necessarily indicate malicious activity, and further investigation would be needed to determine the nature of these requests.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OllamaLLM(model=\"llama3.1:70b\")\n",
    "\n",
    "template = \"\"\"\n",
    "Question: Can you find any anomalies in this data?\n",
    "Data: {data}\n",
    "Answer: There are <number of lines> in this data. I have found the following anomalies on:\n",
    "    <line>: <message>\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "model = OllamaLLM(model=\"llama3.1:70b\", base_url=\"http://g005:11434\")\n",
    "chain = prompt | model\n",
    "chain.invoke({\"data\": results})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLama Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    get_response_synthesizer,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.core.readers.base import BaseReader\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TxtReader(BaseReader):\n",
    "    def load_data(self, file, extra_info=None):\n",
    "        with open(file, \"r\") as f:\n",
    "            text = f.read()\n",
    "        # load_data returns a list of Document objects\n",
    "        return [Document(text=text, extra_info=extra_info or {})]\n",
    "\n",
    "\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_files=[\"llama-data/data/apache_logs/2.txt\"],\n",
    "    file_extractor={\".txt\": TxtReader()},\n",
    ")\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = Ollama(model=\"llama3.1:latest\", request_timeout=360.0)\n",
    "Settings.embed_model = OllamaEmbedding(\n",
    "    model_name=\"llama3.1:70b\",\n",
    "    base_url=\"http://g005:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Response\n"
     ]
    }
   ],
   "source": [
    "# build index\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "retriever = VectorIndexRetriever(index=index, similarity_top_k=100)\n",
    "response_synth = get_response_synthesizer()\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synth,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"How many lines are in the loaded data?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
